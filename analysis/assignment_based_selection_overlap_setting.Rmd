---
title: "assignment_based_selection_overlap_setting"
author: "Annie Xie"
date: "2025-10-07"
output: 
  workflowr::wflow_html:
    code_folding: hide
editor_options:
  chunk_output_type: console
---

# Introduction
In this analysis, I test an assignment-based stability selection method in the sparse overlapping setting.

```{r load_packages, message = FALSE, warning = FALSE}
library(dplyr)
library(ggplot2)
library(pheatmap)
library(flashier)
```

```{r load_viz_code}
source('code/visualization_functions.R')
```

```{r cosine_sim_code}
source('code/compute_cosine_sim_matrix.R')
```

# Stability Selection Code
```{r split_data_code}
stability_selection_split_data <- function(X, dim = c('rows', 'columns')){

  if (dim == 'rows'){
    n <- nrow(X)
    n1 <- ceiling(n/2)
    # n2 <- floor(nrow(X)/2)
  } else if (dim == 'columns'){
    n <- ncol(X)
    n1 <- ceiling(n/2)
    # n2 <- floor(ncol(X)/2)
  } else {
    stop('Wrong input for dim')
  }

  subset1 <- sample(n, size = n1, replace = FALSE)
  if (dim == 'columns'){
    X1 <- X[,subset1]
    X2 <- X[,-(subset1)]
  } else { # dim = 'rows'
    X1 <- X[subset1,]
    X2 <- X[-(subset1),]
  }
  return(list(X1, X2, subset1, setdiff(c(1:n), subset1)))
}
```

```{r}
filter_out_duplicates <- function(L){
  cosine_sim_mat <- compute_cosine_sim_matrix(L,L)
  cosine_sim_mat[lower.tri(cosine_sim_mat, diag = TRUE)] <- 0
  duplicates <- which(cosine_sim_mat > 0.99 ,arr.ind = TRUE)
  
  removed.idx <- c()
  for (i in 1:nrow(duplicates)){
    idx.to_remove <- duplicates[i,2]
    if (!(idx.to_remove %in% removed.idx)){
      L <- L[,-idx.to_remove, drop = FALSE]
      removed.idx <- c(removed.idx, idx.to_remove)
    }
  }
}
```

```{r}
stability_selection_post_processing <- function(L1, L2, threshold=0.99){
  
  # compute similarity
  cosine_sim_matrix <- compute_cosine_sim_matrix(L1, L2)
  
  # find pairings
  # assignment_problem <- lpSolve::lp.assign(cosine_sim_matrix, direction = "max")
  # pairings <- which(assignment_problem$solution == 1, arr.ind = TRUE)
  
  assignment_problem <- RcppHungarian::HungarianSolver(-1*cosine_sim_matrix)
  pairings <- assignment_problem$pairs
  
  # remove things that were not paired
  no_pair <- pairings[,2] == 0
  pairings <- pairings[!(no_pair), , drop = FALSE]
  
  # filter pairings
  pairing_similarities <- cosine_sim_matrix[pairings]
  pairings.keep <- pairing_similarities > threshold
  pairings <- pairings[pairings.keep, , drop = FALSE]
  K <- nrow(pairings)
  
  # compute final estimate
  if (K > 0){
    L_final <- matrix(0, nrow = nrow(L1), ncol = K)
    for(i in 1:K){
      L_final[,i] <- rowMeans(cbind(L1[, pairings[i,1]], L2[, pairings[i,2]]))
    }
  } else {
    L_final <- matrix(0, nrow = nrow(L1), ncol = 1)
  }
  
  return(L_final)
}
```

# Sparse Overlapping Data Generation

```{r}
sim_binary_loadings_data <- function(args) {
  set.seed(args$seed)
  
  FF <- matrix(rnorm(args$k * args$p, sd = args$group_sd), ncol = args$k)
  if (args$constrain_F) {
    FF_svd <- svd(FF)
    FF <- FF_svd$u
    FF <- t(t(FF) * rep(args$group_sd, args$k) * sqrt(p))
  }
  LL <- matrix(rbinom(args$n*args$k, 1, args$pi1), nrow = args$n, ncol = args$k)
  E <- matrix(rnorm(args$n * args$p, sd = args$indiv_sd), nrow = args$n)
  
  Y <- LL %*% t(FF) + E
  YYt <- (1/args$p)*tcrossprod(Y)
  
  return(list(Y = Y, YYt = YYt, LL = LL, FF = FF, K = ncol(LL)))
}
```

```{r}
n <- 100
p <- 1000
k <- 10
pi1 <- 0.1
indiv_sd <- 1
group_sd <- 1
seed <- 1
sim_args = list(n = n, p = p, k = k, pi1 = pi1, indiv_sd = indiv_sd, group_sd = group_sd, seed = seed, constrain_F = FALSE)
sim_data <- sim_binary_loadings_data(sim_args)
```

This is a heatmap of the true loadings matrix:
```{r}
plot_heatmap(sim_data$LL)
```

# EBCD

## Split by Column

```{r}
set.seed(1)
X_split_by_col <- stability_selection_split_data(sim_data$Y, dim = 'columns')
```

```{r}
set.seed(1)
ebcd_fits_by_col <- list()
for (i in 1:(length(X_split_by_col)/2)){
  ebcd_fits_by_col[[i]] <- ebcd::ebcd(t(X_split_by_col[[i]]), 
                                   Kmax = 20, 
                                   ebnm_fn = ebnm::ebnm_generalized_binary)$EL
}
```

This is heatmap of the loadings estimate from the first subset:
```{r}
plot_heatmap(ebcd_fits_by_col[[1]], colors_range = c('blue','gray96','red'), brks = seq(-max(abs(ebcd_fits_by_col[[1]])), max(abs(ebcd_fits_by_col[[1]])), length.out = 50))
```

This is the max correlation for each true factor:
```{r}
apply(cor(ebcd_fits_by_col[[1]], sim_data$LL), 2, max)
```

All of the factors are accurately recovered. The first 10 factors correspond to the true factors and the other ten factors are spurious.

This is a heatmap of the loadings estimate from the second subset:
```{r}
plot_heatmap(ebcd_fits_by_col[[2]], colors_range = c('blue','gray96','red'), brks = seq(-max(abs(ebcd_fits_by_col[[2]])), max(abs(ebcd_fits_by_col[[2]])), length.out = 50))
```

This is the max correlation for each true factor:
```{r}
apply(cor(ebcd_fits_by_col[[2]], sim_data$LL), 2, max)
```

This estimate also accurately recovers all of the factors. Again, the first 10 factors correspond to the true factors and the other ten factors are spurious.

Both loadings estimates have ten extra factors. Visually, these extra factors don't seem to be highly correlated across the two estimates.

```{r}
cosine_sim_mat <- compute_cosine_sim_matrix(ebcd_fits_by_col[[1]], ebcd_fits_by_col[[2]])
```

These are histograms of the cosine similarities for each factor of the first subset estimate:
```{r}
par(mfrow = c(4,5), mar = c(2,2,1,1))
for (i in 1:ncol(ebcd_fits_by_col[[1]])){
  hist(cosine_sim_mat[i,], main = "", xlab = paste('factor',i))
}
par(mfrow = c(1,1))
```

Now, let's apply stability selection. Based off of the histograms of the cosine similarity, I chose a threshold of 0.8. Note that the gap between similar and dissimilar pairs is quite large, so I could have used a lower threshold, e.g. 0.5, and gotten the same results.
```{r}
set.seed(1)
L_est_by_col <- stability_selection_post_processing(ebcd_fits_by_col[[1]], ebcd_fits_by_col[[2]], threshold = 0.8)
```

This is a heatmap of the final loadings estimate:
```{r}
plot_heatmap(L_est_by_col, colors_range = c('blue','gray96','red'), brks = seq(-max(abs(L_est_by_col)), max(abs(L_est_by_col)), length.out = 50))
```

This is the correlation between the estimate and the true loadings:
```{r}
apply(cor(L_est_by_col, sim_data$LL), 1, max)
```

These are the true factors which are most highly correlated with each estimated factor:
```{r}
apply(cor(L_est_by_col, sim_data$LL), 1, which.max)
```

We are able to accurately recover all ten components.

## Split by Row
Now, we try splitting the rows:
```{r}
set.seed(1)
X_split_by_row <- stability_selection_split_data(sim_data$Y, dim = 'rows')
```

```{r}
transform_ebcd_Z <- function(Y, ebcd_obj){
  Y.svd <- svd(Y)
  Y.UV <- Y.svd$u %*% t(Y.svd$v)
  Z_transformed <- Y.UV %*% ebcd_obj$Z
  return(Z_transformed)
}
```

This is the true loadings matrix for the first subset:
```{r}
plot_heatmap(sim_data$LL[X_split_by_row[[3]],])
```

These are the number of samples per factor in the first subset:
```{r}
colSums(sim_data$LL[X_split_by_row[[3]],])
```

We see that for the first subset, there are no samples with loading in the second factor.

This is the true loadings matrix for the second subset:
```{r}
plot_heatmap(sim_data$LL[X_split_by_row[[4]],])
```

These are the number of samples per factor in the second subset:
```{r}
colSums(sim_data$LL[X_split_by_row[[4]],])
```

For the second subset, there are no samples with loading in the first or fifth factor.

```{r}
set.seed(1)
ebcd_fits_by_row_L <- list()
ebcd_fits_by_row_F <- list()
for (i in 1:(length(X_split_by_row)/2)){
  ebcd_fit <- ebcd::ebcd(t(X_split_by_row[[i]]), 
                                   Kmax = 20, 
                                   ebnm_fn = ebnm::ebnm_generalized_binary)
  ebcd_fits_by_row_L[[i]] <- ebcd_fit$EL
  ebcd_fits_by_row_F[[i]] <- transform_ebcd_Z(t(X_split_by_row[[i]]), ebcd_fit)
}
```

This is heatmap of the loadings estimate from the first subset:
```{r}
plot_heatmap(ebcd_fits_by_row_L[[1]][order(X_split_by_row[[3]]),], colors_range = c('blue','gray96','red'), brks = seq(-max(abs(ebcd_fits_by_row_L[[1]])), max(abs(ebcd_fits_by_row_L[[1]])), length.out = 50))
```

This is heatmap of the loadings estimate from the second subset:
```{r}
plot_heatmap(ebcd_fits_by_row_L[[2]], colors_range = c('blue','gray96','red'), brks = seq(-max(abs(ebcd_fits_by_row_L[[2]])), max(abs(ebcd_fits_by_row_L[[2]])), length.out = 50))
```

This is heatmap of the factor estimate from the first subset:
```{r}
plot_heatmap(ebcd_fits_by_row_F[[1]], colors_range = c('blue','gray96','red'), brks = seq(-max(abs(ebcd_fits_by_row_F[[1]])), max(abs(ebcd_fits_by_row_F[[1]])), length.out = 50))
```

This is the correlation between the true factors and the estimated factors:
```{r}
apply(cor(ebcd_fits_by_row_F[[1]], sim_data$FF), 2, max)
```

As expected, the estimate does a poor job at recovering the second true factor (there are no samples with membership in this factor, so there is no information on this factor in this subset).

This is heatmap of the factor estimate from the second subset:
```{r}
plot_heatmap(ebcd_fits_by_row_F[[2]], colors_range = c('blue','gray96','red'), brks = seq(-max(abs(ebcd_fits_by_row_F[[2]])), max(abs(ebcd_fits_by_row_F[[2]])), length.out = 50))
```

This is the correlation between the true factors and the estimated factors:
```{r}
apply(cor(ebcd_fits_by_row_F[[2]], sim_data$FF), 2, max)
```

As expected, the estimate does a poor job at recovering the first and fifth true factors (for the same reasoning as the previous example).

```{r}
cosine_sim_mat <- compute_cosine_sim_matrix(ebcd_fits_by_row_F[[1]], ebcd_fits_by_row_F[[2]])
```

These are histograms of the cosine similarities for each factor of the first subset estimate:
```{r}
par(mfrow = c(5,4), mar = c(2,2,1,1))
for (i in 1:ncol(ebcd_fits_by_row_F[[1]])){
  hist(cosine_sim_mat[i,], main = "", xlab = paste('factor',i))
}
par(mfrow = c(1,1))
```

Now we apply stability selection.

### Threshold = 0.6

Based off of the histograms, I chose a threshold = 0.6.
```{r}
set.seed(1)
F_est <- stability_selection_post_processing(ebcd_fits_by_row_F[[1]], ebcd_fits_by_row_F[[2]], threshold = 0.6)
```

This is a heatmap of the final factor estimate:
```{r}
plot_heatmap(F_est, colors_range = c('blue','gray96','red'), brks = seq(-max(abs(F_est)), max(abs(F_est)), length.out = 50))
```

This is the correlation between the estimated factors and the true factors:
```{r}
apply(cor(F_est, sim_data$FF), 1, max)
```

These are the true factors which are most correlated with each estimated factor:
```{r}
apply(cor(F_est, sim_data$FF), 1, which.max)
```

We are able to recover five components. The method does not return any extra factors.

I obtain an initial estimate for $L$ by using non-negative least squares.
```{r}
# use nonnegative least squares estimate here
L_est <- t(NNLM::nnlm(x = F_est , y = t(sim_data$Y))$coefficients)
```

This is a heatmap of the initial $L$ estimate:
```{r}
plot_heatmap(L_est, colors_range = c('blue','gray96','red'), brks = seq(-max(abs(L_est)), max(abs(L_est)), length.out = 50))
```

Now, we run one iteration of backfit with EBCD.
```{r}
ebcd_fit_L_given_Z <- function(Y, Z, EL, ebnm_fn, maxiter = 1){
  n <- nrow(Y)
  p <- ncol(Y)
  Kmax <- ncol(Z)
  
  # initial estimate for tau
  tau <- prod(dim(Y)) / sum((Y - (Z%*%t(EL)))^2)
  V <- matrix(0, nrow = nrow(EL), ncol = ncol(EL))
  
  for (i in 1:maxiter){
    for (k in 1:Kmax) {
      x <- c(crossprod(Y, Z[, k])) / n
      s <- rep(sqrt(1 / (n * tau)), times=length(x))
      e <- ebnm_fn(x = x, s = s)
      
      EL[, k] <- e$posterior$mean
      V[, k] <- e$posterior$sd^2
    }
    # update tau
    tau <- prod(dim(Y)) / (sum((Y - Z %*% t(EL))^2) + n * sum(V))
  }
  return(EL)
}
```

```{r}
# issue: F is not orthogonal here; is that okay?
ebcd_estimate_L <- ebcd_fit_L_given_Z(t(sim_data$Y), F_est, L_est, ebnm::ebnm_generalized_binary, maxiter = 1)
```

This is a heatmap of the final loadings estimate:
```{r}
plot_heatmap(ebcd_estimate_L)
```

This is the correlation between the estimated loadings and the true loadings:
```{r}
apply(cor(ebcd_estimate_L, sim_data$LL), 1, max)
```

These are the true loadings which are most correlated with each estimated loadings factor:
```{r}
apply(cor(ebcd_estimate_L, sim_data$LL), 1, which.max)
```

We are able to accurately recover five components.

For brevity, I do not include the version of backfitting which uses an orthogonal matrix for the $F$ estimate. But I suspect the results would be about the same.

### Threshold = 0.1

For comparison, I tried applying stability selection with other thresholds. I tested a couple of other thresholds and found that 0.1 was the smallest threshold that did not return any spurious factors.

```{r}
set.seed(1)
F_est <- stability_selection_post_processing(ebcd_fits_by_row_F[[1]], ebcd_fits_by_row_F[[2]], threshold = 0.1)
```

This is a heatmap of the final factor estimate:
```{r}
plot_heatmap(F_est, colors_range = c('blue','gray96','red'), brks = seq(-max(abs(F_est)), max(abs(F_est)), length.out = 50))
```

This is the correlation between the estimated factors and the true factors:
```{r}
apply(cor(F_est, sim_data$FF), 1, max)
```

These are the true factors which are most correlated with each estimated factor:
```{r}
apply(cor(F_est, sim_data$FF), 1, which.max)
```

We are able to recover seven components. The method does not return any extra factors.

I obtain an initial estimate for $L$ by using non-negative least squares.
```{r}
# use nonnegative least squares estimate here
L_est <- t(NNLM::nnlm(x = F_est , y = t(sim_data$Y))$coefficients)
```

This is a heatmap of the initial $L$ estimate:
```{r}
plot_heatmap(L_est, colors_range = c('blue','gray96','red'), brks = seq(-max(abs(L_est)), max(abs(L_est)), length.out = 50))
```

Now, we run one iteration of backfit with EBCD.
```{r}
# issue: F is not orthogonal here; is that okay?
ebcd_estimate_L <- ebcd_fit_L_given_Z(t(sim_data$Y), F_est, L_est, ebnm::ebnm_generalized_binary, maxiter = 1)
```

This is a heatmap of the final loadings estimate:
```{r}
plot_heatmap(ebcd_estimate_L)
```

This is the correlation between the estimated loadings and the true loadings:
```{r}
apply(cor(ebcd_estimate_L, sim_data$LL), 1, max)
```

These are the true loadings which are most correlated with each estimated loadings factor:
```{r}
apply(cor(ebcd_estimate_L, sim_data$LL), 1, which.max)
```

We are able to accurately recover seven components, which is realistically the maximum number of components we can recover. (Recall, there are three factors which are only present in one of the two subsets).

# GBCD

In this section, I test the method with GBCD.

## Split by Column

```{r split_by_col, eval = FALSE}
set.seed(1)
X_split_by_col <- stability_selection_split_data(sim_data$Y, dim = 'columns')
```

```{r gbcd_col_split_fits}
gbcd_fits_by_col <- list()
for (i in 1:(length(X_split_by_col)/2)){
  gbcd_fits_by_col[[i]] <- gbcd::fit_gbcd(X_split_by_col[[i]], 
                                   Kmax = 10, 
                                   prior = ebnm::ebnm_generalized_binary,
                                   verbose = 0)$L
}
```

This is heatmap of the loadings estimate from the first subset:
```{r}
plot_heatmap(gbcd_fits_by_col[[1]], colors_range = c('blue','gray96','red'), brks = seq(-max(abs(gbcd_fits_by_col[[1]])), max(abs(gbcd_fits_by_col[[1]])), length.out = 50))
```

This is the max correlation for each true factor:
```{r}
apply(cor(gbcd_fits_by_col[[1]], sim_data$LL), 2, max)
```

All of the factors are accurately recovered.

This is a heatmap of the loadings estimate from the second subset:
```{r}
plot_heatmap(gbcd_fits_by_col[[2]], colors_range = c('blue','gray96','red'), brks = seq(-max(abs(gbcd_fits_by_col[[2]])), max(abs(gbcd_fits_by_col[[2]])), length.out = 50))
```

This is the max correlation for each true factor:
```{r}
apply(cor(gbcd_fits_by_col[[2]], sim_data$LL), 2, max)
```

True factors 4,6, and 10 are not recovered as well. So I expect the stability selection method will have a harder time recovering these factors.

These are histograms of the similarities:
```{r}
L1_L2_cosine_sim_mat <- compute_cosine_sim_matrix(gbcd_fits_by_col[[1]], gbcd_fits_by_col[[2]])
```

```{r}
par(mfrow = c(4,4), mar = c(2,2,1,1))
for(i in 1:nrow(L1_L2_cosine_sim_mat)){
  hist(L1_L2_cosine_sim_mat[i,], xlab = "", ylab = "", main = paste('factor',i))
}
par(mfrow = c(1,1))
```

Now we apply stability selection. From the histograms above, I decided to apply stability selection with a threshold of 0.8.
```{r}
set.seed(1)
L_est_by_col <- stability_selection_post_processing(gbcd_fits_by_col[[1]], gbcd_fits_by_col[[2]], threshold = 0.8)
```

This is a heatmap of the final loadings estimate:
```{r}
plot_heatmap(L_est_by_col, colors_range = c('blue','gray96','red'), brks = seq(-max(abs(L_est_by_col)), max(abs(L_est_by_col)), length.out = 50))
```

This is the correlation between the estimate and the true loadings:
```{r}
apply(cor(L_est_by_col, sim_data$LL), 1, max)
```

These are the true loadings factors which are most correlated with each estimated loadings factor:
```{r}
apply(cor(L_est_by_col, sim_data$LL), 1, which.max)
```

We are able to recover nine of the ten components. Component 10 was well recovered in the first subset, but not in the second subset. So perhaps the factors that corresponded most with true factor 10 were not paired together, or if they were paired together, their similarity was not high enough. (After further investigation, it seems to be the former.)

## Split by Row
Now, we try splitting the rows:
```{r, eval = FALSE}
set.seed(1)
X_split_by_row <- stability_selection_split_data(sim_data$Y, dim = 'rows')
```

```{r}
gbcd_fits_by_row_F <- list()
gbcd_fits_by_row_L <- list()
for (i in 1:(length(X_split_by_row)/2)){
  gbcd_fit <- gbcd::fit_gbcd(X_split_by_row[[i]], 
                                   Kmax = 10, 
                                   prior = ebnm::ebnm_generalized_binary,
                                   verbose = 0)
  gbcd_fits_by_row_F[[i]] <- gbcd_fit$F$lfc
  gbcd_fits_by_row_L[[i]] <- gbcd_fit$L
}
```

This is heatmap of the factor estimate from the first subset:
```{r}
plot_heatmap(gbcd_fits_by_row_F[[1]], colors_range = c('blue','gray96','red'), brks = seq(-max(abs(gbcd_fits_by_row_F[[1]])), max(abs(gbcd_fits_by_row_F[[1]])), length.out = 50))
```

This is heatmap of the factor estimate from the second subset:
```{r}
plot_heatmap(gbcd_fits_by_row_F[[2]], colors_range = c('blue','gray96','red'), brks = seq(-max(abs(gbcd_fits_by_row_F[[2]])), max(abs(gbcd_fits_by_row_F[[2]])), length.out = 50))
```

To process the factor estimates, I first remove factors which are essentially zero vectors.
```{r}
for (j in 1:2){
  idx.keep <- apply(gbcd_fits_by_row_F[[j]], 2, function(x){sqrt(sum(x^2))}) > 10^(-10)
  gbcd_fits_by_row_F[[j]] <- gbcd_fits_by_row_F[[j]][,idx.keep]
}
```

I also normalize all of factors to have l2 norm = 1.
```{r}
gbcd_fits_by_row_F <- lapply(gbcd_fits_by_row_F, function(x){t(t(x)/sqrt(colSums(x^2)))})
```

This is heatmap of the new factor estimate from the first subset:
```{r}
plot_heatmap(gbcd_fits_by_row_F[[1]], colors_range = c('blue','gray96','red'), brks = seq(-max(abs(gbcd_fits_by_row_F[[1]])), max(abs(gbcd_fits_by_row_F[[1]])), length.out = 50))
```

This is heatmap of the new factor estimate from the second subset:
```{r}
plot_heatmap(gbcd_fits_by_row_F[[2]], colors_range = c('blue','gray96','red'), brks = seq(-max(abs(gbcd_fits_by_row_F[[2]])), max(abs(gbcd_fits_by_row_F[[2]])), length.out = 50))
```

These are histograms of the similarities:
```{r}
F1_F2_cosine_sim_mat <- compute_cosine_sim_matrix(gbcd_fits_by_row_F[[1]], gbcd_fits_by_row_F[[2]])
```

```{r}
par(mfrow = c(4,4), mar = c(2,2,1,1))
for(i in 1:nrow(F1_F2_cosine_sim_mat)){
  hist(F1_F2_cosine_sim_mat[i,], xlab = "", ylab = "", main = paste('factor',i))
}
par(mfrow = c(1,1))
```

Now, we apply stability selection. 

### Threshold = 0.6
After analyzing the cosine similarity matrix and histograms, I chose a similarity threshold of 0.6.
```{r}
set.seed(1)
F_est <- stability_selection_post_processing(gbcd_fits_by_row_F[[1]], gbcd_fits_by_row_F[[2]], threshold = 0.6)
```

This is a heatmap of the final factor estimate:
```{r}
plot_heatmap(F_est, colors_range = c('blue','gray96','red'), brks = seq(-max(abs(F_est)), max(abs(F_est)), length.out = 50))
```

This is the correlation between the estimated factors and true factors:
```{r}
apply(cor(F_est, sim_data$FF), 1, max)
```

These are the true factors that are most correlated with each estimated factor:
```{r}
apply(cor(F_est, sim_data$FF), 1, which.max)
```

We recover four of the ten factors. 

I obtain an initial estimate for $L$ using non-negative least squares.
```{r}
# use nonnegative least squares estimate here
L_est <- t(NNLM::nnlm(x = F_est , y = t(sim_data$Y))$coefficients)
```

```{r}
plot_heatmap(L_est, colors_range = c('blue','gray96','red'), brks = seq(-max(abs(L_est)), max(abs(L_est)), length.out = 50))
```

Now, we run one iteration of backfit with flash. I use a generalized binary prior on the loadings.
```{r}
flash_estimate_L <- flash_init(sim_data$Y) %>%
  flash_factors_init(list(L_est, F_est), ebnm_fn = c(ebnm::ebnm_generalized_binary, ebnm::ebnm_point_laplace)) %>%
  flash_factors_fix(kset = 1:ncol(F_est), which_dim = 'factors') %>%
  flash_backfit(maxiter = 1) %>%
  flash_nullcheck()
```

This is a heatmap of the final loadings estimate:
```{r}
plot_heatmap(flash_estimate_L$L_pm)
```

The one backfit iteration helps make the estimate more sparse.

This is the correlation between the estimated loadings and the true loadings:
```{r}
apply(cor(flash_estimate_L$L_pm, sim_data$LL), 1, max)
```

These are the true loadings which are most correlated with each estimated loadings factor:
```{r}
apply(cor(flash_estimate_L$L_pm, sim_data$LL), 1, which.max)
```

### Threshold = 0.1
For comparison, I also tried a threshold of 0.1. (I also tried other thresholds, and this is the smallest one I found that did not return any extra factors).
```{r}
set.seed(1)
F_est <- stability_selection_post_processing(gbcd_fits_by_row_F[[1]], gbcd_fits_by_row_F[[2]], threshold = 0.1)
```

This is a heatmap of the final factor estimate:
```{r}
plot_heatmap(F_est, colors_range = c('blue','gray96','red'), brks = seq(-max(abs(F_est)), max(abs(F_est)), length.out = 50))
```

This is the correlation between the estimated factors and true factors:
```{r}
apply(cor(F_est, sim_data$FF), 1, max)
```

These are the true factors that are most correlated with each estimated factor:
```{r}
apply(cor(F_est, sim_data$FF), 1, which.max)
```

We recover seven of the ten factors. 

I obtain an initial estimate for $L$ using non-negative least squares.
```{r}
# use nonnegative least squares estimate here
L_est <- t(NNLM::nnlm(x = F_est , y = t(sim_data$Y))$coefficients)
```

```{r}
plot_heatmap(L_est, colors_range = c('blue','gray96','red'), brks = seq(-max(abs(L_est)), max(abs(L_est)), length.out = 50))
```

Now, we run one iteration of backfit with flash.
```{r}
flash_estimate_L <- flash_init(sim_data$Y) %>%
  flash_factors_init(list(L_est, F_est), ebnm_fn = c(ebnm::ebnm_generalized_binary, ebnm::ebnm_point_laplace)) %>%
  flash_factors_fix(kset = 1:ncol(F_est), which_dim = 'factors') %>%
  flash_backfit(maxiter = 1) %>%
  flash_nullcheck()
```

This is a heatmap of the final loadings estimate:
```{r}
plot_heatmap(flash_estimate_L$L_pm)
```

Again, the one backfit iteration helps make the estimate more sparse, shrinking the small values to zero.

This is the correlation between the estimated loadings and the true loadings:
```{r}
apply(cor(flash_estimate_L$L_pm, sim_data$LL), 1, max)
```

These are the true loadings which are most correlated with each estimated loadings factor:
```{r}
apply(cor(flash_estimate_L$L_pm, sim_data$LL), 1, which.max)
```

# CoDesymNMF

## Split by Columns

```{r, eval = FALSE}
set.seed(1)
X_split_by_col <- stability_selection_split_data(sim_data$Y, dim = 'columns')
```

```{r}
set.seed(1)
codesymnmf_fits_by_col <- list()
for (i in 1:(length(X_split_by_col)/2)){
  S <- tcrossprod(X_split_by_col[[i]])/ncol(X_split_by_col[[i]])
  codesymnmf_fits_by_col[[i]] <- codesymnmf::codesymnmf(S, 20)$H
}
```

This is heatmap of the loadings estimate from the first subset:
```{r}
plot_heatmap(codesymnmf_fits_by_col[[1]], colors_range = c('blue','gray96','red'), brks = seq(-max(abs(codesymnmf_fits_by_col[[1]])), max(abs(codesymnmf_fits_by_col[[1]])), length.out = 50))
```

This is the correlation of the true factors and estimated factors:
```{r}
apply(cor(codesymnmf_fits_by_col[[1]], sim_data$LL), 2, max)
```

All the true factors are recovered well in this subset.

This is a heatmap of the loadings estimate from the second subset:
```{r}
plot_heatmap(codesymnmf_fits_by_col[[2]], colors_range = c('blue','gray96','red'), brks = seq(-max(abs(codesymnmf_fits_by_col[[2]])), max(abs(codesymnmf_fits_by_col[[2]])), length.out = 50))
```

This is the correlation of the true factors and estimated factors:
```{r}
apply(cor(codesymnmf_fits_by_col[[2]], sim_data$LL), 2, max)
```

All the true factors are recovered well in this subset.

These are histograms of the similarities:
```{r}
L1_L2_cosine_sim_mat <- compute_cosine_sim_matrix(codesymnmf_fits_by_col[[1]], codesymnmf_fits_by_col[[2]])
```

```{r}
par(mfrow = c(5,4), mar = c(2,2,1,1))
for(i in 1:nrow(L1_L2_cosine_sim_mat)){
  hist(L1_L2_cosine_sim_mat[i,], xlab = "", ylab = "", main = paste('factor',i))
}
par(mfrow = c(1,1))
```

Now, let's apply stability selection. 

### Threshold = 0.8

Based off the histograms, I picked a threshold of 0.8
```{r}
set.seed(1)
L_est_by_col <- stability_selection_post_processing(codesymnmf_fits_by_col[[1]], codesymnmf_fits_by_col[[2]], threshold = 0.8)
```

This is a heatmap of the final loadings estimate:
```{r}
plot_heatmap(L_est_by_col, colors_range = c('blue','gray96','red'), brks = seq(-max(abs(L_est_by_col)), max(abs(L_est_by_col)), length.out = 50))
```

This is the correlation between the estimate and the true loadings:
```{r}
apply(cor(L_est_by_col, sim_data$LL), 1, max)
```

This is the true factor which is most correlated with each estimated factor:
```{r}
apply(cor(L_est_by_col, sim_data$LL), 1, which.max)
```

We are able to recover all ten components. However, we also return two extra components -- there are two estimated components which are most correlated with true component 10 and two estimated components which are most correlated with true component 6.

### Threshold = 0.88

I tested other threshold options and found that 0.88 was the smallest value which did not return more than 10 factors. However, not all of the true signals are recovered. In addition, if we increase the threshold to 0.9, then the method returns 8 factors.

```{r}
set.seed(1)
L_est_by_col <- stability_selection_post_processing(codesymnmf_fits_by_col[[1]], codesymnmf_fits_by_col[[2]], threshold = 0.88)
```

This is a heatmap of the final loadings estimate:
```{r}
plot_heatmap(L_est_by_col, colors_range = c('blue','gray96','red'), brks = seq(-max(abs(L_est_by_col)), max(abs(L_est_by_col)), length.out = 50))
```

This is the correlation between the estimate and the true loadings:
```{r}
apply(cor(L_est_by_col, sim_data$LL), 1, max)
```

This is the true factor which is most correlated with each estimated factor:
```{r}
apply(cor(L_est_by_col, sim_data$LL), 1, which.max)
```

We see that there are two components which are most correlated with true component 6. Looking at the correlation matrix, we see that true component 4 is not well recovered, with a maximum correlation of 0.31. This is surprising since both of the subset estimates seemed to recover component 4 well. Looking into this further, it seems like the components that had the highest correlation with true component 4 were not matched. As a result, a lower threshold is needed to recover this component.
