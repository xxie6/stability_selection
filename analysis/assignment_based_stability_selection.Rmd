---
title: "assignment_based_stability_selection"
author: "Annie Xie"
date: "2025-10-03"
output: 
  workflowr::wflow_html:
    code_folding: hide
editor_options:
  chunk_output_type: console
---

# Introduction

In this analysis, I try a stability selection method similar to Consensus NMF (with two splits).

```{r load_packages, message = FALSE, warning = FALSE}
library(dplyr)
library(ggplot2)
library(pheatmap)
library(flashier)
```

```{r load_viz_code}
source('code/visualization_functions.R')
```

```{r cosine_sim_code}
compute_cosine_sim_matrix <- function(L1, L2){
  norms1 <- apply(L1, 2, function(x){sqrt(sum(x^2))})
  norms1[norms1 == 0] <- Inf

  norms2 <- apply(L2, 2, function(x){sqrt(sum(x^2))})
  norms2[norms2 == 0] <- Inf

  L1_normalized <- t(t(L1)/norms1)
  L2_normalized <- t(t(L2)/norms2)

  #compute matrix of cosine similarities
  cosine_sim_matrix <- crossprod(L1_normalized, L2_normalized)

  return(cosine_sim_matrix)
}
```

# Stability Selection Code
```{r split_data_code}
stability_selection_split_data <- function(X, dim = c('rows', 'columns')){

  if (dim == 'rows'){
    n <- nrow(X)
    n1 <- ceiling(n/2)
    # n2 <- floor(nrow(X)/2)
  } else if (dim == 'columns'){
    n <- ncol(X)
    n1 <- ceiling(n/2)
    # n2 <- floor(ncol(X)/2)
  } else {
    stop('Wrong input for dim')
  }

  subset1 <- sample(n, size = n1, replace = FALSE)
  if (dim == 'columns'){
    X1 <- X[,subset1]
    X2 <- X[,-(subset1)]
  } else { # dim = 'rows'
    X1 <- X[subset1,]
    X2 <- X[-(subset1),]
  }
  return(list(X1, X2, subset1, setdiff(c(1:n), subset1)))
}
```

```{r filter_duplicates_code}
filter_out_duplicates <- function(L){
  cosine_sim_mat <- compute_cosine_sim_matrix(L,L)
  cosine_sim_mat[lower.tri(cosine_sim_mat, diag = TRUE)] <- 0
  duplicates <- which(cosine_sim_mat > 0.99 ,arr.ind = TRUE)
  
  removed.idx <- c()
  for (i in 1:nrow(duplicates)){
    idx.to_remove <- duplicates[i,2]
    if (!(idx.to_remove %in% removed.idx)){
      L <- L[,-idx.to_remove, drop = FALSE]
      removed.idx <- c(removed.idx, idx.to_remove)
    }
  }
}
```

```{r stability_selection_code}
stability_selection_post_processing <- function(L1, L2, threshold=0.99){
  
  # compute similarity
  cosine_sim_matrix <- compute_cosine_sim_matrix(L1, L2)
  
  # find pairings
  # assignment_problem <- lpSolve::lp.assign(cosine_sim_matrix, direction = "max")
  # pairings <- which(assignment_problem$solution == 1, arr.ind = TRUE)
  
  assignment_problem <- RcppHungarian::HungarianSolver(-1*cosine_sim_matrix)
  pairings <- assignment_problem$pairs
  
  # remove things that were not paired
  no_pair <- pairings[,2] == 0
  pairings <- pairings[!(no_pair), , drop = FALSE]
  
  # filter pairings
  pairing_similarities <- cosine_sim_matrix[pairings]
  pairings.keep <- pairing_similarities > threshold
  pairings <- pairings[pairings.keep, , drop = FALSE]
  K <- nrow(pairings)
  
  # compute final estimate
  if (K > 0){
    L_final <- matrix(0, nrow = nrow(L1), ncol = K)
    for(i in 1:K){
      L_final[,i] <- rowMeans(cbind(L1[, pairings[i,1]], L2[, pairings[i,2]]))
    }
  } else {
    L_final <- matrix(0, nrow = nrow(L1), ncol = 1)
  }
  
  return(L_final)
}
```

# Unbalanced Nonoverlap

In this section, I test out the method in the unbalanced non-overlapping setting.

```{r sim_star_code}
sim_star_data <- function(args) {
  set.seed(args$seed)
  
  n <- sum(args$pop_sizes)
  p <- args$n_genes
  K <- length(args$pop_sizes)
  
  FF <- matrix(rnorm(K * p, sd = rep(args$branch_sds, each = p)), ncol = K)
  
  LL <- matrix(0, nrow = n, ncol = K)
  for (k in 1:K) {
    vec <- rep(0, K)
    vec[k] <- 1
    LL[, k] <- rep(vec, times = args$pop_sizes)
  }
  
  E <- matrix(rnorm(n * p, sd = args$indiv_sd), nrow = n)
  Y <- LL %*% t(FF) + E
  YYt <- (1/p)*tcrossprod(Y)
  
  return(list(Y = Y, YYt = YYt, LL = LL, FF = FF, K = ncol(LL)))
}
```

```{r sim_star_data}
pop_sizes <- c(20,50,30,60)
n_genes <- 1000
branch_sds <- rep(2,4)
indiv_sd <- 1
seed <- 1
sim_args = list(pop_sizes = pop_sizes, branch_sds = branch_sds, indiv_sd = indiv_sd, n_genes = n_genes, seed = seed)
sim_data <- sim_star_data(sim_args)
```

This is a heatmap of the true loadings matrix:
```{r unbal_star_true_loadings}
plot_heatmap(sim_data$LL)
```

## GBCD

In this section, I test the method with GBCD.

### Split by Column

```{r split_by_col}
set.seed(1)
X_split_by_col <- stability_selection_split_data(sim_data$Y, dim = 'columns')
```

```{r gbcd_col_split_fits}
gbcd_fits_by_col <- list()
for (i in 1:(length(X_split_by_col)/2)){
  gbcd_fits_by_col[[i]] <- gbcd::fit_gbcd(X_split_by_col[[i]], 
                                   Kmax = 4, 
                                   prior = ebnm::ebnm_generalized_binary,
                                   verbose = 0)$L
}
```

This is heatmap of the loadings estimate from the first subset:
```{r}
plot_heatmap(gbcd_fits_by_col[[1]], colors_range = c('blue','gray96','red'), brks = seq(-max(abs(gbcd_fits_by_col[[1]])), max(abs(gbcd_fits_by_col[[1]])), length.out = 50))
```

This is a heatmap of the loadings estimate from the second subset:
```{r}
plot_heatmap(gbcd_fits_by_col[[2]], colors_range = c('blue','gray96','red'), brks = seq(-max(abs(gbcd_fits_by_col[[2]])), max(abs(gbcd_fits_by_col[[2]])), length.out = 50))
```

The first four factors of the estimates are highly similar. The second estimate has three extra factors.

These are histograms of the similarities:
```{r}
L1_L2_cosine_sim_mat <- compute_cosine_sim_matrix(gbcd_fits_by_col[[1]], gbcd_fits_by_col[[2]])
```

```{r}
par(mfrow = c(2,2))
for(i in 1:nrow(L1_L2_cosine_sim_mat)){
  hist(L1_L2_cosine_sim_mat[i,], xlab = "", ylab = "", main = paste('factor',i))
}
par(mfrow = c(1,1))
```

Now we apply stability selection. From the histograms above, I decided to apply stability selection with a threshold of 0.8.
```{r}
set.seed(1)
L_est_by_col <- stability_selection_post_processing(gbcd_fits_by_col[[1]], gbcd_fits_by_col[[2]], threshold = 0.8)
```

This is a heatmap of the final loadings estimate:
```{r}
plot_heatmap(L_est_by_col, colors_range = c('blue','gray96','red'), brks = seq(-max(abs(L_est_by_col)), max(abs(L_est_by_col)), length.out = 50))
```

This is the correlation between the estimate and the true loadings:
```{r}
apply(cor(L_est_by_col, sim_data$LL), 1, max)
```

We are able to recover all four components.

### Split by Row
Now, we try splitting the rows:
```{r}
set.seed(1)
X_split_by_row <- stability_selection_split_data(sim_data$Y, dim = 'rows')
```

```{r}
gbcd_fits_by_row_F <- list()
gbcd_fits_by_row_L <- list()
for (i in 1:(length(X_split_by_row)/2)){
  gbcd_fit <- gbcd::fit_gbcd(X_split_by_row[[i]], 
                                   Kmax = 4, 
                                   prior = ebnm::ebnm_generalized_binary,
                                   verbose = 0)
  gbcd_fits_by_row_F[[i]] <- gbcd_fit$F$lfc
  gbcd_fits_by_row_L[[i]] <- gbcd_fit$L
}
```

This is heatmap of the factor estimate from the first subset:
```{r}
plot_heatmap(gbcd_fits_by_row_F[[1]], colors_range = c('blue','gray96','red'), brks = seq(-max(abs(gbcd_fits_by_row_F[[1]])), max(abs(gbcd_fits_by_row_F[[1]])), length.out = 50))
```

This is heatmap of the factor estimate from the second subset:
```{r}
plot_heatmap(gbcd_fits_by_row_F[[2]], colors_range = c('blue','gray96','red'), brks = seq(-max(abs(gbcd_fits_by_row_F[[2]])), max(abs(gbcd_fits_by_row_F[[2]])), length.out = 50))
```

For both estimates, the last two factors appear to be close to the zero vector. Let's take a closer look.

This is a plot of the last two factor estimates from the first subset:
```{r}
par(mfrow = c(1,2))
for (i in 5:6){
  plot(gbcd_fits_by_row_F[[1]][,i], ylab = 'Factor Estimate')
}
par(mfrow = c(1,1))
```

This is a plot of the last two factor estimates from the first subset:
```{r}
par(mfrow = c(1,2))
for (i in 5:6){
  plot(gbcd_fits_by_row_F[[2]][,i], ylab = 'Factor Estimate')
}
par(mfrow = c(1,1))
```

Some of these factors are zero vectors. So we should remove these. (It seems like GBCD does not have a check for whether factors are zero. I think this is something that Joon came across once?).
```{r}
for (j in 1:2){
  idx.keep <- apply(gbcd_fits_by_row_F[[j]], 2, function(x){sqrt(sum(x^2))}) > 10^(-10)
  gbcd_fits_by_row_F[[j]] <- gbcd_fits_by_row_F[[j]][,idx.keep]
}
```

I also normalize all of factors to have l2 norm = 1.
```{r}
gbcd_fits_by_row_F <- lapply(gbcd_fits_by_row_F, function(x){t(t(x)/sqrt(colSums(x^2)))})
```

This is heatmap of the new factor estimate from the first subset:
```{r}
plot_heatmap(gbcd_fits_by_row_F[[1]], colors_range = c('blue','gray96','red'), brks = seq(-max(abs(gbcd_fits_by_row_F[[1]])), max(abs(gbcd_fits_by_row_F[[1]])), length.out = 50))
```

This is heatmap of the new factor estimate from the second subset:
```{r}
plot_heatmap(gbcd_fits_by_row_F[[2]], colors_range = c('blue','gray96','red'), brks = seq(-max(abs(gbcd_fits_by_row_F[[2]])), max(abs(gbcd_fits_by_row_F[[2]])), length.out = 50))
```

These are histograms of the similarities:
```{r}
F1_F2_cosine_sim_mat <- compute_cosine_sim_matrix(gbcd_fits_by_row_F[[1]], gbcd_fits_by_row_F[[2]])
```

```{r}
par(mfrow = c(2,2))
for(i in 1:nrow(F1_F2_cosine_sim_mat)){
  hist(F1_F2_cosine_sim_mat[i,], xlab = "", ylab = "", main = paste('factor',i))
}
par(mfrow = c(1,1))
```

This is the cosine similarity matrix:
```{r}
F1_F2_cosine_sim_mat
```

Now, we apply stability selection. After analyzing the cosine similarity matrix and histograms, I again chose a similarity threshold of 0.8.
```{r}
set.seed(1)
F_est <- stability_selection_post_processing(gbcd_fits_by_row_F[[1]], gbcd_fits_by_row_F[[2]], threshold = 0.8)
```

This is a heatmap of the final factor estimate:
```{r}
plot_heatmap(F_est, colors_range = c('blue','gray96','red'), brks = seq(-max(abs(F_est)), max(abs(F_est)), length.out = 50))
```

This is the correlation between the estimated factors and true factors:
```{r}
apply(cor(F_est, sim_data$FF), 1, max)
```

These are the true factors that are most correlated with each estimated factor:
```{r}
apply(cor(F_est, sim_data$FF), 1, which.max)
```

We are able to accurately recover all four factors.

I obtain an initial estimate for $L$ using non-negative least squares.
```{r}
# use nonnegative least squares estimate here
L_est <- t(NNLM::nnlm(x = F_est , y = t(sim_data$Y))$coefficients)
```

```{r}
plot_heatmap(L_est, colors_range = c('blue','gray96','red'), brks = seq(-max(abs(L_est)), max(abs(L_est)), length.out = 50))
```

The $L$ estimate has the main components. Now, I run backfitting with flash on the data matrix with $F$ fixed to the consensus estimate to get an estimate for $L$. I use the generalized binary prior for $L$. Note I run the backfit for only one iteration.

```{r}
flash_estimate_L <- flash_init(sim_data$Y) %>%
  flash_factors_init(list(L_est, F_est), ebnm_fn = c(ebnm::ebnm_generalized_binary, ebnm::ebnm_point_laplace)) %>%
  flash_factors_fix(kset = 1:ncol(F_est), which_dim = 'factors') %>%
  flash_backfit(maxiter = 1) %>%
  flash_nullcheck()
```

```{r}
plot_heatmap(flash_estimate_L$L_pm)
```

The one backfit iteration helps make the estimate more sparse, shrinking the small values to zero.

## EBCD
In this section, I try the method with EBCD.

### Split by Column

```{r, eval = FALSE}
set.seed(1)
X_split_by_col <- stability_selection_split_data(sim_data$Y, dim = 'columns')
```

```{r}
set.seed(1)
ebcd_fits_by_col <- list()
for (i in 1:(length(X_split_by_col)/2)){
  ebcd_fits_by_col[[i]] <- ebcd::ebcd(t(X_split_by_col[[i]]), 
                                   Kmax = 8, 
                                   ebnm_fn = ebnm::ebnm_generalized_binary)$EL
}
```

This is heatmap of the loadings estimate from the first subset:
```{r}
plot_heatmap(ebcd_fits_by_col[[1]], colors_range = c('blue','gray96','red'), brks = seq(-max(abs(ebcd_fits_by_col[[1]])), max(abs(ebcd_fits_by_col[[1]])), length.out = 50))
```

This is a heatmap of the loadings estimate from the second subset:
```{r}
plot_heatmap(ebcd_fits_by_col[[2]], colors_range = c('blue','gray96','red'), brks = seq(-max(abs(ebcd_fits_by_col[[2]])), max(abs(ebcd_fits_by_col[[2]])), length.out = 50))
```

Factor pairs (1,4), (2,1), (3,3), and (4,2) are highly similar. Both estimates have four extra factors which visually don't seem to correlate with each other.

These are histograms of the similarities:
```{r}
L1_L2_cosine_sim_mat <- compute_cosine_sim_matrix(ebcd_fits_by_col[[1]], ebcd_fits_by_col[[2]])
```

```{r}
par(mfrow = c(2,4))
for(i in 1:nrow(L1_L2_cosine_sim_mat)){
  hist(L1_L2_cosine_sim_mat[i,], xlab = "", ylab = "", main = paste('factor',i))
}
par(mfrow = c(1,1))
```

Now, let's apply stability selection. Based off the histograms above, I chose a similarity threshold of 0.8.
```{r}
set.seed(1)
L_est_by_col <- stability_selection_post_processing(ebcd_fits_by_col[[1]], ebcd_fits_by_col[[2]], threshold = 0.8)
```

This is a heatmap of the final loadings estimate:
```{r}
plot_heatmap(L_est_by_col, colors_range = c('blue','gray96','red'), brks = seq(-max(abs(L_est_by_col)), max(abs(L_est_by_col)), length.out = 50))
```

This is the correlation between the estimate and the true loadings:
```{r}
apply(cor(L_est_by_col, sim_data$LL), 1, max)
```

We are able to recover all four components.

### Split by Row

Now, we try splitting the rows:
```{r, eval = FALSE}
set.seed(1)
X_split_by_row <- stability_selection_split_data(sim_data$Y, dim = 'rows')
```

```{r}
transform_ebcd_Z <- function(Y, ebcd_obj){
  Y.svd <- svd(Y)
  Y.UV <- Y.svd$u %*% t(Y.svd$v)
  Z_transformed <- Y.UV %*% ebcd_obj$Z
  return(Z_transformed)
}
```

```{r}
set.seed(1)
ebcd_fits_by_row_L <- list()
ebcd_fits_by_row_F <- list()
for (i in 1:(length(X_split_by_row)-2)){
  ebcd_fit <- ebcd::ebcd(t(X_split_by_row[[i]]), 
                                   Kmax = 8, 
                                   ebnm_fn = ebnm::ebnm_generalized_binary)
  ebcd_fits_by_row_L[[i]] <- ebcd_fit$EL
  ebcd_fits_by_row_F[[i]] <- transform_ebcd_Z(t(X_split_by_row[[i]]), ebcd_fit)
}
```

This is heatmap of the factor estimate from the first subset:
```{r}
plot_heatmap(ebcd_fits_by_row_F[[1]], colors_range = c('blue','gray96','red'), brks = seq(-max(abs(ebcd_fits_by_row_F[[1]])), max(abs(ebcd_fits_by_row_F[[1]])), length.out = 50))
```

This is heatmap of the factor estimate from the second subset:
```{r}
plot_heatmap(ebcd_fits_by_row_F[[2]], colors_range = c('blue','gray96','red'), brks = seq(-max(abs(ebcd_fits_by_row_F[[2]])), max(abs(ebcd_fits_by_row_F[[2]])), length.out = 50))
```

These are histograms of the similarities:
```{r}
F1_F2_cosine_sim_mat <- compute_cosine_sim_matrix(ebcd_fits_by_row_F[[1]], ebcd_fits_by_row_F[[2]])
```

```{r}
par(mfrow = c(2,4))
for(i in 1:nrow(F1_F2_cosine_sim_mat)){
  hist(F1_F2_cosine_sim_mat[i,], xlab = "", ylab = "", main = paste('factor',i))
}
par(mfrow = c(1,1))
```

Let's apply stability selection. Again, based off the histograms above, I chose a threshold of 0.8.
```{r}
set.seed(1)
F_est <- stability_selection_post_processing(ebcd_fits_by_row_F[[1]], ebcd_fits_by_row_F[[2]], threshold = 0.8)
```

This is a heatmap of the final factor estimate:
```{r}
plot_heatmap(F_est, colors_range = c('blue','gray96','red'), brks = seq(-max(abs(F_est)), max(abs(F_est)), length.out = 50))
```

This is the correlation between the estimated factors and the true factors:
```{r}
apply(cor(F_est, sim_data$FF), 1, max)
```

These are the true factors which are most correlated with each estimated factor:
```{r}
apply(cor(F_est, sim_data$FF), 1, which.max)
```

We are able to accurately recover the four factors.

Now, I obtain an initial estimate for $L$ by using non-negative least squares.
```{r}
# use nonnegative least squares estimate here
L_est <- t(NNLM::nnlm(x = F_est , y = t(sim_data$Y))$coefficients)
```

```{r}
plot_heatmap(L_est, colors_range = c('blue','gray96','red'), brks = seq(-max(abs(L_est)), max(abs(L_est)), length.out = 50))
```

The loadings estimate contains the main components. Now, I run backfit with EBCD for one iteration. I use the generalized binary prior on the loadings.

```{r}
ebcd_fit_L_given_Z <- function(Y, Z, EL, ebnm_fn, maxiter = 1){
  n <- nrow(Y)
  p <- ncol(Y)
  Kmax <- ncol(Z)
  
  # initial estimate for tau
  tau <- prod(dim(Y)) / sum((Y - (Z%*%t(EL)))^2)
  V <- matrix(0, nrow = nrow(EL), ncol = ncol(EL))
  
  for (i in 1:maxiter){
    for (k in 1:Kmax) {
      x <- c(crossprod(Y, Z[, k])) / n
      s <- rep(sqrt(1 / (n * tau)), times=length(x))
      e <- ebnm_fn(x = x, s = s)
      
      EL[, k] <- e$posterior$mean
      V[, k] <- e$posterior$sd^2
    }
    # update tau
    tau <- prod(dim(Y)) / (sum((Y - Z %*% t(EL))^2) + n * sum(V))
  }
  return(EL)
}
```

```{r}
# issue: F is not orthogonal here; is that okay?
ebcd_estimate_L <- ebcd_fit_L_given_Z(t(sim_data$Y), F_est, L_est, ebnm::ebnm_generalized_binary, maxiter = 1)
```

This is a heatmap of the loadings estimate:
```{r}
plot_heatmap(ebcd_estimate_L, colors_range = c('blue','gray96','red'), brks = seq(-max(abs(ebcd_estimate_L)), max(abs(ebcd_estimate_L)), length.out = 50))
```

One caveat is the factor estimate is not exactly orthogonal. So I try a procedure where I compute the closest orthogonal matrix for our $F$ estimate, and then use that to obtain an estimate for $L$. Given that the entries of the true factor matrix are drawn from independent, identically-distributed normal distributions, I expect that the $F$ estimate is close to orthogonal. So I don't expect the results to change. But it is arguably more principled to work with an orthogonal $F$ when applying backfit with EBCD.

I compute the closest orthogonal matrix to our estimate of $F$:
```{r}
PolarU <- function(A) {
  svdA <- svd(A)
  out <- svdA$u %*% t(svdA$v)
  return(out)
}
```

```{r}
F_est_orthog <- PolarU(F_est/sqrt(80))*sqrt(160)
```

Now, we obtain an estimate for $L$ using non-negative least squares:
```{r}
# use nonnegative least squares estimate here
L_est_orthog <- t(NNLM::nnlm(x = F_est_orthog , y = t(sim_data$Y))$coefficients)
```

This is a heatmap of the $L$ estimate.
```{r}
plot_heatmap(L_est_orthog, colors_range = c('blue','gray96','red'), brks = seq(-max(abs(L_est_orthog)), max(abs(L_est_orthog)), length.out = 50))
```

Now, we apply one iteration of backfit with EBCD.
```{r}
ebcd_estimate_L_orthog <- ebcd_fit_L_given_Z(t(sim_data$Y), F_est_orthog, L_est_orthog, ebnm::ebnm_generalized_binary, maxiter = 1)
```

This is a heatmap of the final loadings estimate:
```{r}
plot_heatmap(ebcd_estimate_L_orthog, colors_range = c('blue','gray96','red'), brks = seq(-max(abs(ebcd_estimate_L_orthog)), max(abs(ebcd_estimate_L_orthog)), length.out = 50))
```

The results do not drastically change.

# Balanced Nonoverlap

In this section, I test out the method in the balanced non-overlapping setting.

```{r}
pop_sizes <- rep(40,4)
n_genes <- 1000
branch_sds <- rep(2,4)
indiv_sd <- 1
seed <- 1
sim_args = list(pop_sizes = pop_sizes, branch_sds = branch_sds, indiv_sd = indiv_sd, n_genes = n_genes, seed = seed)
sim_data <- sim_star_data(sim_args)
```

This is a heatmap of the true loadings matrix:
```{r}
plot_heatmap(sim_data$LL)
```

## GBCD

### Split by Column

```{r}
set.seed(1)
X_split_by_col <- stability_selection_split_data(sim_data$Y, dim = 'columns')
```

```{r}
gbcd_fits_by_col <- list()
for (i in 1:(length(X_split_by_col)/2)){
  gbcd_fits_by_col[[i]] <- gbcd::fit_gbcd(X_split_by_col[[i]], 
                                   Kmax = 4, 
                                   prior = ebnm::ebnm_generalized_binary,
                                   verbose = 0)$L
}
```

This is heatmap of the loadings estimate from the first subset:
```{r}
plot_heatmap(gbcd_fits_by_col[[1]], colors_range = c('blue','gray96','red'), brks = seq(-max(abs(gbcd_fits_by_col[[1]])), max(abs(gbcd_fits_by_col[[1]])), length.out = 50))
```

This is a heatmap of the loadings estimate from the second subset:
```{r}
plot_heatmap(gbcd_fits_by_col[[2]], colors_range = c('blue','gray96','red'), brks = seq(-max(abs(gbcd_fits_by_col[[2]])), max(abs(gbcd_fits_by_col[[2]])), length.out = 50))
```

An interesting observation is that GBCD yields duplicate factors (In my experience, it usually does not). I wonder if the corresponding factors are zero factors.

These are histograms of the similarities:
```{r}
L1_L2_cosine_sim_mat <- compute_cosine_sim_matrix(gbcd_fits_by_col[[1]], gbcd_fits_by_col[[2]])
```

```{r}
par(mfrow = c(3,2))
for(i in 1:nrow(L1_L2_cosine_sim_mat)){
  hist(L1_L2_cosine_sim_mat[i,], xlab = "", ylab = "", main = paste('factor',i))
}
par(mfrow = c(1,1))
```

Now we apply stability selection. I again chose a similarity threshold of 0.8.
```{r}
set.seed(1)
L_est_by_col <- stability_selection_post_processing(gbcd_fits_by_col[[1]], gbcd_fits_by_col[[2]], threshold = 0.8)
```

This is a heatmap of the final loadings estimate:
```{r}
plot_heatmap(L_est_by_col, colors_range = c('blue','gray96','red'), brks = seq(-max(abs(L_est_by_col)), max(abs(L_est_by_col)), length.out = 50))
```

This is the correlation between the estimate and the true loadings:
```{r}
apply(cor(L_est_by_col, sim_data$LL), 1, max)
```

We are able to recover all four components. However, we did pick up one of the duplicate factors since both estimates had duplicate factors for the third population. So we may need a procedure to get rid of duplicates.

### Split by Row
Now, we try splitting the rows:
```{r}
set.seed(1)
X_split_by_row <- stability_selection_split_data(sim_data$Y, dim = 'rows')
```

```{r}
gbcd_fits_by_row_F <- list()
gbcd_fits_by_row_L <- list()
for (i in 1:(length(X_split_by_row)/2)){
  gbcd_fit <- gbcd::fit_gbcd(X_split_by_row[[i]], 
                                   Kmax = 4, 
                                   prior = ebnm::ebnm_generalized_binary,
                                   verbose = 0)
  gbcd_fits_by_row_F[[i]] <- gbcd_fit$F$lfc
  gbcd_fits_by_row_L[[i]] <- gbcd_fit$L
}
```

This is heatmap of the factor estimate from the first subset:
```{r}
plot_heatmap(gbcd_fits_by_row_F[[1]], colors_range = c('blue','gray96','red'), brks = seq(-max(abs(gbcd_fits_by_row_F[[1]])), max(abs(gbcd_fits_by_row_F[[1]])), length.out = 50))
```

This is heatmap of the factor estimate from the second subset:
```{r}
plot_heatmap(gbcd_fits_by_row_F[[2]], colors_range = c('blue','gray96','red'), brks = seq(-max(abs(gbcd_fits_by_row_F[[2]])), max(abs(gbcd_fits_by_row_F[[2]])), length.out = 50))
```

For both estimates, the last two factors appear to be close to the zero vector. Let's take a closer look.

This is a plot of the last two factor estimates from the first subset:
```{r}
par(mfrow = c(1,2))
for (i in 5){
  plot(gbcd_fits_by_row_F[[1]][,i], ylab = 'Factor Estimate')
}
par(mfrow = c(1,1))
```

This is a plot of the last three factor estimates from the first subset:
```{r}
par(mfrow = c(2,2))
for (i in 5:7){
  plot(gbcd_fits_by_row_F[[2]][,i], ylab = 'Factor Estimate')
}
par(mfrow = c(1,1))
```

Similar to the previous analyses, we remove the factors which are essentially zero vectors.

```{r}
for (j in 1:2){
  idx.keep <- apply(gbcd_fits_by_row_F[[j]], 2, function(x){sqrt(sum(x^2))}) > 10^(-10)
  gbcd_fits_by_row_F[[j]] <- gbcd_fits_by_row_F[[j]][,idx.keep]
}
```

I also normalize all of factors to have l2 norm = 1.
```{r}
gbcd_fits_by_row_F <-lapply(gbcd_fits_by_row_F, function(x){t(t(x)/sqrt(colSums(x^2)))})
```

This is heatmap of the new factor estimate from the first subset:
```{r}
plot_heatmap(gbcd_fits_by_row_F[[1]], colors_range = c('blue','gray96','red'), brks = seq(-max(abs(gbcd_fits_by_row_F[[1]])), max(abs(gbcd_fits_by_row_F[[1]])), length.out = 50))
```

This is heatmap of the new factor estimate from the second subset:
```{r}
plot_heatmap(gbcd_fits_by_row_F[[2]], colors_range = c('blue','gray96','red'), brks = seq(-max(abs(gbcd_fits_by_row_F[[2]])), max(abs(gbcd_fits_by_row_F[[2]])), length.out = 50))
```

These are histograms of the similarities:
```{r}
F1_F2_cosine_sim_mat <- compute_cosine_sim_matrix(gbcd_fits_by_row_F[[1]], gbcd_fits_by_row_F[[2]])
```

```{r}
par(mfrow = c(2,2))
for(i in 1:nrow(F1_F2_cosine_sim_mat)){
  hist(F1_F2_cosine_sim_mat[i,], xlab = "", ylab = "", main = paste('factor',i))
}
par(mfrow = c(1,1))
```

This is the similarity matrix:
```{r}
F1_F2_cosine_sim_mat
```

Now, we apply stability selection. I again chose a similarity threshold of 0.8.
```{r}
set.seed(1)
F_est <- stability_selection_post_processing(gbcd_fits_by_row_F[[1]], gbcd_fits_by_row_F[[2]], threshold = 0.8)
```

This is a heatmap of the final factor estimate:
```{r}
plot_heatmap(F_est, colors_range = c('blue','gray96','red'), brks = seq(-max(abs(F_est)), max(abs(F_est)), length.out = 50))
```

This is the correlation between the estimated factors and the true factors:
```{r}
apply(cor(F_est, sim_data$FF), 1, max)
```

These are the true factors which are most correlated with each estimated factor:
```{r}
apply(cor(F_est, sim_data$FF), 1, which.max)
```

We are able to accurately recover all four factors.

Now, we obtain an estimate for $L$ using non-negative least squares.
```{r}
# use nonnegative least squares estimate here
L_est <- t(NNLM::nnlm(x = F_est , y = t(sim_data$Y))$coefficients)
```

```{r}
plot_heatmap(L_est, colors_range = c('blue','gray96','red'), brks = seq(-max(abs(L_est)), max(abs(L_est)), length.out = 50))
```

The $L$ estimate has the main components. Now, I run backfitting with flash on the data matrix with $F$ fixed to the consensus estimate to get an estimate for $L$. I use the generalized binary prior for $L$.

```{r}
flash_estimate_L <- flash_init(sim_data$Y) %>%
  flash_factors_init(list(L_est, F_est), ebnm_fn = c(ebnm::ebnm_generalized_binary, ebnm::ebnm_point_laplace)) %>%
  flash_factors_fix(kset = 1:ncol(F_est), which_dim = 'factors') %>%
  flash_backfit(maxiter = 1) %>%
  flash_nullcheck()
```

```{r}
plot_heatmap(flash_estimate_L$L_pm)
```

The one backfit iteration helps make the estimate more sparse, shrinking the small values to zero.

## EBCD

### Split by Column

```{r, eval = FALSE}
set.seed(1)
X_split_by_col <- stability_selection_split_data(sim_data$Y, dim = 'columns')
```

```{r}
set.seed(1)
ebcd_fits_by_col <- list()
for (i in 1:(length(X_split_by_col)/2)){
  ebcd_fits_by_col[[i]] <- ebcd::ebcd(t(X_split_by_col[[i]]), 
                                   Kmax = 8, 
                                   ebnm_fn = ebnm::ebnm_generalized_binary)$EL
}
```

This is heatmap of the loadings estimate from the first subset:
```{r}
plot_heatmap(ebcd_fits_by_col[[1]], colors_range = c('blue','gray96','red'), brks = seq(-max(abs(ebcd_fits_by_col[[1]])), max(abs(ebcd_fits_by_col[[1]])), length.out = 50))
```

This is a heatmap of the loadings estimate from the second subset:
```{r}
plot_heatmap(ebcd_fits_by_col[[2]], colors_range = c('blue','gray96','red'), brks = seq(-max(abs(ebcd_fits_by_col[[2]])), max(abs(ebcd_fits_by_col[[2]])), length.out = 50))
```

These are histograms of the similarities:
```{r}
L1_L2_cosine_sim_mat <- compute_cosine_sim_matrix(ebcd_fits_by_col[[1]], ebcd_fits_by_col[[2]])
```

```{r}
par(mfrow = c(2,4))
for(i in 1:nrow(L1_L2_cosine_sim_mat)){
  hist(L1_L2_cosine_sim_mat[i,], xlab = "", ylab = "", main = paste('factor',i))
}
par(mfrow = c(1,1))
```

Now, let's apply stability selection. I again use a similarity threshold of 0.8.
```{r}
set.seed(1)
L_est_by_col <- stability_selection_post_processing(ebcd_fits_by_col[[1]], ebcd_fits_by_col[[2]], threshold = 0.8)
```

This is a heatmap of the final loadings estimate:
```{r}
plot_heatmap(L_est_by_col, colors_range = c('blue','gray96','red'), brks = seq(-max(abs(L_est_by_col)), max(abs(L_est_by_col)), length.out = 50))
```

This is the correlation between the estimate and the true loadings:
```{r}
apply(cor(L_est_by_col, sim_data$LL), 1, max)
```

We are able to recover all four components.

### Split by Row
Now, we try splitting the rows:
```{r, eval = FALSE}
set.seed(1)
X_split_by_row <- stability_selection_split_data(sim_data$Y, dim = 'rows')
```

```{r, eval = FALSE, include = FALSE}
transform_ebcd_Z <- function(Y, ebcd_obj){
  Y.svd <- svd(Y)
  Y.UV <- Y.svd$u %*% t(Y.svd$v)
  Z_transformed <- Y.UV %*% ebcd_obj$Z
  return(Z_transformed)
}
```

```{r}
set.seed(1)
ebcd_fits_by_row_L <- list()
ebcd_fits_by_row_F <- list()
for (i in 1:(length(X_split_by_row)-2)){
  ebcd_fit <- ebcd::ebcd(t(X_split_by_row[[i]]), 
                                   Kmax = 8, 
                                   ebnm_fn = ebnm::ebnm_generalized_binary)
  ebcd_fits_by_row_L[[i]] <- ebcd_fit$EL
  ebcd_fits_by_row_F[[i]] <- transform_ebcd_Z(t(X_split_by_row[[i]]), ebcd_fit)
}
```

This is heatmap of the factor estimate from the first subset:
```{r}
plot_heatmap(ebcd_fits_by_row_F[[1]], colors_range = c('blue','gray96','red'), brks = seq(-max(abs(ebcd_fits_by_row_F[[1]])), max(abs(ebcd_fits_by_row_F[[1]])), length.out = 50))
```

This is heatmap of the factor estimate from the second subset:
```{r}
plot_heatmap(ebcd_fits_by_row_F[[2]], colors_range = c('blue','gray96','red'), brks = seq(-max(abs(ebcd_fits_by_row_F[[2]])), max(abs(ebcd_fits_by_row_F[[2]])), length.out = 50))
```

These are histograms of the similarities:
```{r}
F1_F2_cosine_sim_mat <- compute_cosine_sim_matrix(ebcd_fits_by_row_F[[1]], ebcd_fits_by_row_F[[2]])
```

```{r}
par(mfrow = c(2,4))
for(i in 1:nrow(F1_F2_cosine_sim_mat)){
  hist(F1_F2_cosine_sim_mat[i,], xlab = "", ylab = "", main = paste('factor',i))
}
par(mfrow = c(1,1))
```

Let's apply stability selection
```{r}
set.seed(1)
F_est <- stability_selection_post_processing(ebcd_fits_by_row_F[[1]], ebcd_fits_by_row_F[[2]], threshold = 0.8)
```

This is a heatmap of the final factor estimate:
```{r}
plot_heatmap(F_est, colors_range = c('blue','gray96','red'), brks = seq(-max(abs(F_est)), max(abs(F_est)), length.out = 50))
```

This is the correlation between the estimated factors and the true factors:
```{r}
apply(cor(F_est, sim_data$FF), 1, max)
```

These are the true factors which are most correlated with each estimated factor:
```{r}
apply(cor(F_est, sim_data$FF), 1, which.max)
```

I obtain an initial estimate for $L$ by using non-negative least squares.

```{r}
# use nonnegative least squares estimate here
L_est <- t(NNLM::nnlm(x = F_est , y = t(sim_data$Y))$coefficients)
```

This is a heatmap of the loadings estimate:
```{r}
plot_heatmap(L_est, colors_range = c('blue','gray96','red'), brks = seq(-max(abs(L_est)), max(abs(L_est)), length.out = 50))
```

Now, I run one backfit iteration with EBCD.

```{r, eval = FALSE, include = FALSE}
ebcd_fit_L_given_Z <- function(Y, Z, EL, ebnm_fn, maxiter = 1){
  n <- nrow(Y)
  p <- ncol(Y)
  Kmax <- ncol(Z)
  
  # initial estimate for tau
  tau <- prod(dim(Y)) / sum((Y - (Z%*%t(EL)))^2)
  V <- matrix(0, nrow = nrow(EL), ncol = ncol(EL))
  
  for (i in 1:maxiter){
    for (k in 1:Kmax) {
      x <- c(crossprod(Y, Z[, k])) / n
      s <- rep(sqrt(1 / (n * tau)), times=length(x))
      e <- ebnm_fn(x = x, s = s)
      
      EL[, k] <- e$posterior$mean
      V[, k] <- e$posterior$sd^2
    }
    # update tau
    tau <- prod(dim(Y)) / (sum((Y - Z %*% t(EL))^2) + n * sum(V))
  }
  return(EL)
}
```

```{r}
# issue: F is not orthogonal here; is that okay?
ebcd_estimate_L <- ebcd_fit_L_given_Z(t(sim_data$Y), F_est, L_est, ebnm::ebnm_generalized_binary, maxiter = 1)
```

This is a heatmap of the final loadings estimate.
```{r}
plot_heatmap(ebcd_estimate_L, colors_range = c('blue','gray96','red'), brks = seq(-max(abs(ebcd_estimate_L)), max(abs(ebcd_estimate_L)), length.out = 50))
```

This is the version of the $L$ estimation procedure which uses an orthogonal $F$ estimate:
```{r, eval = FALSE, include = FALSE}
PolarU <- function(A) {
  svdA <- svd(A)
  out <- svdA$u %*% t(svdA$v)
  return(out)
}
```

```{r}
F_est_orthog <- PolarU(F_est/sqrt(80))*sqrt(160)
```

```{r}
# use nonnegative least squares estimate here
L_est_orthog <- t(NNLM::nnlm(x = F_est_orthog , y = t(sim_data$Y))$coefficients)
```

This is a heatmap of the loadings estimate from non-negative least squares:
```{r}
plot_heatmap(L_est_orthog, colors_range = c('blue','gray96','red'), brks = seq(-max(abs(L_est_orthog)), max(abs(L_est_orthog)), length.out = 50))
```

```{r}
ebcd_estimate_L_orthog <- ebcd_fit_L_given_Z(t(sim_data$Y), F_est_orthog, L_est_orthog, ebnm::ebnm_generalized_binary, maxiter = 1)
```

This is the final loadings estimate:
```{r}
plot_heatmap(ebcd_estimate_L_orthog, colors_range = c('blue','gray96','red'), brks = seq(-max(abs(ebcd_estimate_L_orthog)), max(abs(ebcd_estimate_L_orthog)), length.out = 50))
```
