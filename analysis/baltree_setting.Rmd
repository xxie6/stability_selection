---
title: "baltree_setting"
author: "Annie Xie"
date: "2025-09-11"
output: 
  workflowr::wflow_html:
    code_folding: hide
editor_options:
  chunk_output_type: console
---

# Introduction
In this analysis, we are interested in testing stability selection approaches in the balanced tree setting. I am curious to see how the stability selection does in this setting because many methods struggle due to the non-identifiability issues. I'm curious to see if the subsetting of the data leads to different loadings estimates. Furthermore, will that lead to only a couple of factors being returned?

At a high level, the stability selection involves 1) splitting the data into two subsets, 2) applying the method to each subset, 3) choosing the components that have high correspondence across the two sets of results. We will test two different approaches to step 1). The first approach is splitting the data by splitting the columns. This approach feels intuitive since we are interested in the loadings matrix, which says something about the samples in the dataset. In a population genetics application, one could argue that all of the chromosomes are undergoing evolution independently, and so you could split the data by even vs. odd chromosomes to get two different datasets. However, in a single-cell RNA-seq application, it feels more natural to split the data by cells -- this feels more like creating sub-datasets compared to splitting by genes (unless you want to make some assumption that the genes are pulled from a "population", but I think that feels less natural). This motivates the second approach: splitting the data by splitting the rows.

```{r, message = FALSE, warning = FALSE}
library(dplyr)
library(ggplot2)
library(pheatmap)
```

```{r}
source('code/visualization_functions.R')
source('code/stability_selection_functions.R')
```


# Data Generation

In this analysis, we will focus on the balanced tree setting.
```{r}
sim_4pops <- function(args) {
  set.seed(args$seed)
  
  n <- sum(args$pop_sizes)
  p <- args$n_genes
  
  FF <- matrix(rnorm(7 * p, sd = rep(args$branch_sds, each = p)), ncol = 7)
  # if (args$constrain_F) {
  #   FF_svd <- svd(FF)
  #   FF <- FF_svd$u
  #   FF <- t(t(FF) * branch_sds * sqrt(p))
  # }
  
  LL <- matrix(0, nrow = n, ncol = 7)
  LL[, 1] <- 1
  LL[, 2] <- rep(c(1, 1, 0, 0), times = args$pop_sizes)
  LL[, 3] <- rep(c(0, 0, 1, 1), times = args$pop_sizes)
  LL[, 4] <- rep(c(1, 0, 0, 0), times = args$pop_sizes)
  LL[, 5] <- rep(c(0, 1, 0, 0), times = args$pop_sizes)
  LL[, 6] <- rep(c(0, 0, 1, 0), times = args$pop_sizes)
  LL[, 7] <- rep(c(0, 0, 0, 1), times = args$pop_sizes)
  
  E <- matrix(rnorm(n * p, sd = args$indiv_sd), nrow = n)
  Y <- LL %*% t(FF) + E
  YYt <- (1/p)*tcrossprod(Y)
  return(list(Y = Y, YYt = YYt, LL = LL, FF = FF, K = ncol(LL)))
}
```

```{r}
sim_args = list(pop_sizes = rep(40, 4), n_genes = 1000, branch_sds = rep(2,7), indiv_sd = 1, seed = 1)
sim_data <- sim_4pops(sim_args)
```

This is a heatmap of the true loadings matrix:
```{r}
plot_heatmap(sim_data$LL)
```

# GBCD
In this section, I try stability selection with the GBCD method. In my experiments, I've found that GBCD tends to return extra factors (partially because the point-Laplace initialization will yield extra factors). GBCD usually does a good job at recovering the correct number of components in the tree setting, so I expect the method will still return most, if not all of the factors.

## Stability Selection via Splitting Columns

First, I try splitting the data by splitting the columns.
```{r}
set.seed(1)
X_split_by_col <- stability_selection_split_data(sim_data$Y, dim = 'columns')
```

```{r}
gbcd_fits_by_col <- list()
for (i in 1:(length(X_split_by_col)-2)){
  gbcd_fits_by_col[[i]] <- gbcd::fit_gbcd(X_split_by_col[[i]], 
                                   Kmax = 7, 
                                   prior = ebnm::ebnm_generalized_binary,
                                   verbose = 0)$L
}
```

This is heatmap of the loadings estimate from the first subset:
```{r}
plot_heatmap(gbcd_fits_by_col[[1]], colors_range = c('blue','gray96','red'), brks = seq(-max(abs(gbcd_fits_by_col[[1]])), max(abs(gbcd_fits_by_col[[1]])), length.out = 50))
```

This is a heatmap of the loadings estimate from the second subset:
```{r}
plot_heatmap(gbcd_fits_by_col[[2]], colors_range = c('blue','gray96','red'), brks = seq(-max(abs(gbcd_fits_by_col[[2]])), max(abs(gbcd_fits_by_col[[2]])), length.out = 50))
```

```{r}
results_by_col <- stability_selection_post_processing(gbcd_fits_by_col[[1]], gbcd_fits_by_col[[2]], threshold = 0.8)
L_est_by_col <- results_by_col$L
```

This is the similarity matrix:
```{r}
results_by_col$similarity
```

This is a heatmap of the final loadings estimate:
```{r}
plot_heatmap(L_est_by_col, colors_range = c('blue','gray96','red'), brks = seq(-max(abs(L_est_by_col)), max(abs(L_est_by_col)), length.out = 50))
```

This is the cosine similarity between the estimate and the true loadings matrix:
```{r}
est_true_cos_sim <- compute_cosine_sim_matrix(L_est_by_col, sim_data$LL)
apply(abs(est_true_cos_sim), 1, max)
```

In this case, the method was able to recover all seven factors with high accuracy.

## Stability Selection via Splitting Rows

Now, we try splitting the rows:
```{r}
set.seed(1)
X_split_by_row <- stability_selection_split_data(sim_data$Y, dim = 'rows')
```

```{r}
gbcd_fits_by_row_F <- list()
gbcd_fits_by_row_L <- list()
for (i in 1:(length(X_split_by_row)-2)){
  gbcd_fit <- gbcd::fit_gbcd(X_split_by_row[[i]], 
                                   Kmax = 4, 
                                   prior = ebnm::ebnm_generalized_binary,
                                   verbose = 0)
  gbcd_fits_by_row_F[[i]] <- gbcd_fit$F$lfc
  gbcd_fits_by_row_L[[i]] <- gbcd_fit$L
}
```

This is heatmap of the factor estimate from the first subset:
```{r}
plot_heatmap(gbcd_fits_by_row_L[[1]][order(X_split_by_row[[3]]),], colors_range = c('blue','gray96','red'), brks = seq(-max(abs(gbcd_fits_by_row_L[[1]])), max(abs(gbcd_fits_by_row_L[[1]])), length.out = 50))
```

This is heatmap of the factor estimate from the second subset:
```{r}
plot_heatmap(gbcd_fits_by_row_L[[2]], colors_range = c('blue','gray96','red'), brks = seq(-max(abs(gbcd_fits_by_row_L[[2]])), max(abs(gbcd_fits_by_row_L[[2]])), length.out = 50))
```

```{r}
results_by_row <- stability_selection_row_split_post_processing(gbcd_fits_by_row_L[[1]], gbcd_fits_by_row_L[[2]], gbcd_fits_by_row_F[[1]], gbcd_fits_by_row_F[[2]], threshold = 0.5)
L_est_by_row <- results_by_row$L
L_est_by_row <- L_est_by_row[order(c(X_split_by_row[[3]], X_split_by_row[[4]])), ]
```

This is the similarity matrix:
```{r}
results_by_row$similarity
```

This is a heatmap of the final loadings estimate:
```{r}
plot_heatmap(L_est_by_row, colors_range = c('blue','gray96','red'), brks = seq(-max(abs(L_est_by_row)), max(abs(L_est_by_row)), length.out = 50))
```

This is a plot of the final loadings estimate:
```{r}
plot_loadings(L_est_by_row, Pop = rep(c('A','B','C','D'), each = 40))
```

This is the cosine similarity between the estimate and the true loadings matrix:
```{r}
est_true_cos_sim <- compute_cosine_sim_matrix(L_est_by_row, sim_data$LL)
apply(abs(est_true_cos_sim), 1, max)
```

For comparison, this is the correlation between the estimate and the true loadings matrix (excluding the baseline):
```{r}
apply(cor(L_est_by_row, sim_data$LL[,-1]), 1, max)
```

In this section, five factors are picked up. However, a few of these factors are not at sparse as the true loadings estimate. I'm kind of surprised by the lack of sparsity since the estimates for each half are sparse. I guess combining the two estimates into one loadings matrix resulted in something less sparse? Would it be beneficial to run this fit through flash for a couple of iterations to try to get a sparser $L$?

# EBCD

In this section, I try stability selection with the GBCD method. When given a `Kmax` value that is larger than the true number of components, I've found that EBCD usually returns extra factors. So in this section, when I run EBCD, I give the method double the true number of components.

## Stability Selection via Splitting Columns

```{r}
set.seed(1)
ebcd_fits_by_col <- list()
for (i in 1:(length(X_split_by_col)-2)){
  ebcd_fits_by_col[[i]] <- ebcd::ebcd(t(X_split_by_col[[i]]), 
                                   Kmax = 14, 
                                   ebnm_fn = ebnm::ebnm_generalized_binary)$EL
}
```

This is a heatmap of the estimated loadings from the first subset:
```{r}
plot_heatmap(ebcd_fits_by_col[[1]], colors_range = c('blue','gray96','red'), brks = seq(-max(abs(ebcd_fits_by_col[[1]])), max(abs(ebcd_fits_by_col[[1]])), length.out = 50))
```

This is a heatmap of the estimated loadings from the second subset:
```{r}
plot_heatmap(ebcd_fits_by_col[[2]], colors_range = c('blue','gray96','red'), brks = seq(-max(abs(ebcd_fits_by_col[[2]])), max(abs(ebcd_fits_by_col[[2]])), length.out = 50))
```

```{r}
results_by_col <- stability_selection_post_processing(ebcd_fits_by_col[[1]], ebcd_fits_by_col[[2]], threshold = 0.5)
L_est_by_col <- results_by_col$L
```

This is the similarity matrix:
```{r}
results_by_col$similarity
```

For comparison, this is the correlation between the two estimates:
```{r}
cor(ebcd_fits_by_col[[1]], ebcd_fits_by_col[[2]])
```

This is a heatmap of the final loadings estimate:
```{r}
plot_heatmap(L_est_by_col, colors_range = c('blue','gray96','red'), brks = seq(-max(abs(L_est_by_col)), max(abs(L_est_by_col)), length.out = 50))
```

This is the cosine similarity between the estimate and the true loadings matrix:
```{r}
est_true_cos_sim <- compute_cosine_sim_matrix(L_est_by_col, sim_data$LL)
apply(abs(est_true_cos_sim), 1, max)
```

With a threshold of 0.5, the method returns four factors. However, with a higher threshold, the method wouldn't return any factors.

## Stability Selection via Splitting Rows

```{r}
transform_ebcd_Z <- function(Y, ebcd_obj){
  Y.svd <- svd(Y)
  Y.UV <- Y.svd$u %*% t(Y.svd$v)
  Z_transformed <- Y.UV %*% ebcd_obj$Z
  return(Z_transformed)
}
```

```{r}
set.seed(1)
ebcd_fits_by_row_L <- list()
ebcd_fits_by_row_F <- list()
for (i in 1:(length(X_split_by_row)-2)){
  ebcd_fit <- ebcd::ebcd(t(X_split_by_row[[i]]), 
                                   Kmax = 8, 
                                   ebnm_fn = ebnm::ebnm_generalized_binary)
  ebcd_fits_by_row_L[[i]] <- ebcd_fit$EL
  ebcd_fits_by_row_F[[i]] <- transform_ebcd_Z(t(X_split_by_row[[i]]), ebcd_fit)
}
```

This is a heatmap of the loadings estimate from the first subset:
```{r}
plot_heatmap(ebcd_fits_by_row_L[[1]][order(X_split_by_row[[3]]),], colors_range = c('blue','gray96','red'), brks = seq(-max(abs(ebcd_fits_by_row_L[[1]])), max(abs(ebcd_fits_by_row_L[[1]])), length.out = 50))
```

This is a heatmap of the loadings estimate from the second subset:
```{r}
plot_heatmap(ebcd_fits_by_row_L[[2]], colors_range = c('blue','gray96','red'), brks = seq(-max(abs(ebcd_fits_by_row_L[[2]])), max(abs(ebcd_fits_by_row_L[[2]])), length.out = 50))
```

```{r}
results_by_row <- stability_selection_row_split_post_processing(ebcd_fits_by_row_L[[1]], ebcd_fits_by_row_L[[2]], ebcd_fits_by_row_F[[1]], ebcd_fits_by_row_F[[2]], threshold = 0.5)
L_est_by_row <- results_by_row$L
L_est_by_row <- L_est_by_row[order(c(X_split_by_row[[3]], X_split_by_row[[4]])), ]
```

This is the similarity matrix:
```{r}
results_by_row$similarity
```

This is a heatmap of the final loadings estimate:
```{r}
plot_heatmap(L_est_by_row, colors_range = c('blue','gray96','red'), brks = seq(-max(abs(L_est_by_row)), max(abs(L_est_by_row)), length.out = 50))
```

This is the cosine similarity between the estimate and the true loadings matrix:
```{r}
est_true_cos_sim <- compute_cosine_sim_matrix(L_est_by_row, sim_data$LL)
apply(abs(est_true_cos_sim), 1, max)
```

With a threshold of 0.5, this method returns four factors. These factors look different from what was returned from the column-split stability selection. These factors seem to be less sparse (similar to GBCD with the row-split stability selection). Perhaps when I combine the loadings from the two estimates, the loadings that are getting combined do not exactly look alike, e.g. loadings 3 from estimate 1 is getting paired with loadings 2 from estimate 2, which is resulting in an estimate that in "in-between" two components. So maybe matching the factors in this case is not working as well? In this case, all of the factor entries are drawn from the same normal distribution. So maybe two unrelated factors are similar by chance? This is something to look into/consider more carefully.

# CoDesymNMF

In this section, I try stability selection with the CoDesymNMF method. Similar to EBCD, when given a `Kmax` value that is larger than the true number of components, the method usually returns extra factors. Note that in this section, when I run CoDesymNMF, I give the method double the true number of components.

## Stability Selection via Splitting Columns

```{r}
codesymnmf_fits_by_col <- list()
for (i in 1:(length(X_split_by_col)-2)){
  cov_mat <- tcrossprod(X_split_by_col[[i]])/ncol(X_split_by_col[[i]])
  codesymnmf_fits_by_col[[i]] <- codesymnmf::codesymnmf(cov_mat, 14)$H
}
```

This is a heatmap of the estimated loadings from the first subset:
```{r}
plot_heatmap(codesymnmf_fits_by_col[[1]], colors_range = c('blue','gray96','red'), brks = seq(-max(abs(codesymnmf_fits_by_col[[1]])), max(abs(codesymnmf_fits_by_col[[1]])), length.out = 50))
```

This is a heatmap of the estimated loadings from the second subset:
```{r}
plot_heatmap(codesymnmf_fits_by_col[[2]], colors_range = c('blue','gray96','red'), brks = seq(-max(abs(codesymnmf_fits_by_col[[2]])), max(abs(codesymnmf_fits_by_col[[2]])), length.out = 50))
```

```{r}
results_by_col <- stability_selection_post_processing(codesymnmf_fits_by_col[[1]], codesymnmf_fits_by_col[[2]], threshold = 0.8)
L_est_by_col <- results_by_col$L
```

This is the similarity matrix:
```{r}
results_by_col$similarity
```

This is a heatmap of the final loadings estimate:
```{r}
plot_heatmap(L_est_by_col, colors_range = c('blue','gray96','red'), brks = seq(-max(abs(L_est_by_col)), max(abs(L_est_by_col)), length.out = 50))
```

This is the cosine similarity between the estimate and the true loadings matrix:
```{r}
est_true_cos_sim <- compute_cosine_sim_matrix(L_est_by_col, sim_data$LL)
apply(abs(est_true_cos_sim), 1, max)
```

The method recovers six factors. The factors kind of look like some of the tree factors. However, these factors are not as sparse (as expected since CoDesymNMF does not explicitly encourage sparsity).

## Stability Selection via Splitting Rows

```{r}
PolarU <- function(A) {
  svdA <- svd(A)
  out <- svdA$u %*% t(svdA$v)
  return(out)
}
```

```{r}
codesymnmf_fits_by_row_L <- list()
codesymnmf_fits_by_row_F <- list()
for (i in 1:(length(X_split_by_row)-2)){
  cov_mat <- tcrossprod(X_split_by_row[[i]])/ncol(X_split_by_row[[i]])
  codesymnmf_fits_by_row_L[[i]] <- codesymnmf::codesymnmf(cov_mat, 8)$H
  codesymnmf_fits_by_row_F[[i]] <- PolarU(t(X_split_by_row[[i]]) %*% codesymnmf_fits_by_row_L[[i]])
}
```

This is a heatmap of the estimated loadings from the first subset:
```{r}
plot_heatmap(codesymnmf_fits_by_row_L[[1]][order(X_split_by_row[[3]]),], colors_range = c('blue','gray96','red'), brks = seq(-max(abs(codesymnmf_fits_by_row_L[[1]])), max(abs(codesymnmf_fits_by_row_L[[1]])), length.out = 50))
```

This is a heatmap of the estimated loadings from the second subset:
```{r}
plot_heatmap(codesymnmf_fits_by_row_L[[2]], colors_range = c('blue','gray96','red'), brks = seq(-max(abs(codesymnmf_fits_by_row_L[[2]])), max(abs(codesymnmf_fits_by_row_L[[2]])), length.out = 50))
```

```{r}
results_by_row <- stability_selection_row_split_post_processing(codesymnmf_fits_by_row_L[[1]], codesymnmf_fits_by_row_L[[2]], codesymnmf_fits_by_row_F[[1]], codesymnmf_fits_by_row_F[[2]], threshold = 0.6)
L_est_by_row <- results_by_row$L
L_est_by_row <- L_est_by_row[order(c(X_split_by_row[[3]], X_split_by_row[[4]])), ]
```

This is the similarity matrix:
```{r}
results_by_row$similarity
```

This is a heatmap of the final loadings estimate:
```{r}
plot_heatmap(L_est_by_row, colors_range = c('blue','gray96','red'), brks = seq(-max(abs(L_est_by_row)), max(abs(L_est_by_row)), length.out = 50))
```

This is the cosine similarity between the estimate and the true loadings matrix:
```{r}
est_true_cos_sim <- compute_cosine_sim_matrix(L_est_by_row, sim_data$LL)
apply(abs(est_true_cos_sim), 1, max)
```

The method returns five components.

# Observations

The balanced tree setting is (unsurprisingly) more difficult than the nonoverlapping settings. We also saw some instances where the row-split stability selection performed worse than the column-split stability selection. In particular, the row-split stability selection returned fewer factors and the factors it did return were less sparse. The reduction in sparsity is likely related to how I combined the two loadings estimates from the two halves to create a single loadings estimate for the whole dataset.

